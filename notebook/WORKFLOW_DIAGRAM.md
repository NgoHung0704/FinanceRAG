# ğŸ“Š Dataset-Specific Chunking Evaluation Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATASET-SPECIFIC CHUNKING PIPELINE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“š Input Datasets  â”‚
â”‚  (7 datasets)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€ 1. TATQA (100% tables, avg 2433 chars)
           â”œâ”€â”€â”€ 2. MultiHeirTT (67% tables, hierarchical, avg 2956 chars)
           â”œâ”€â”€â”€ 3. ConvFinQA (100% tables+text, very long avg 4526 chars)
           â”œâ”€â”€â”€ 4. FinQA (100% tables+text, avg 4394 chars)
           â”œâ”€â”€â”€ 5. FinanceBench (0% tables, pure text, avg 1359 chars)
           â”œâ”€â”€â”€ 6. FINDER (0% tables, very short avg 576 chars, large corpus)
           â””â”€â”€â”€ 7. FinQABench (30% tables, mixed, small corpus 92 docs)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ” DATASET CHARACTERIZATION                           â”‚
â”‚  - Detect table presence (%, structure type)                            â”‚
â”‚  - Calculate document lengths (avg, distribution)                       â”‚
â”‚  - Identify content type (pure table / hybrid / pure text)              â”‚
â”‚  - Corpus size analysis                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ§ª DATASET-SPECIFIC STRATEGY SELECTION                     â”‚
â”‚                                                                          â”‚
â”‚  TABLE-HEAVY DATASETS:                                                  â”‚
â”‚  â”œâ”€ TATQA, MultiHeirTT, ConvFinQA, FinQA                               â”‚
â”‚  â””â”€ Test: no_chunking, preserve_tables(3000), preserve_tables(4096)    â”‚
â”‚                                                                          â”‚
â”‚  TEXT-HEAVY DATASETS:                                                   â”‚
â”‚  â”œâ”€ FinanceBench                                                        â”‚
â”‚  â””â”€ Test: recursive(512), recursive(768), recursive(1024)              â”‚
â”‚                                                                          â”‚
â”‚  SHORT-DOC DATASETS:                                                    â”‚
â”‚  â”œâ”€ FINDER, FinQABench                                                 â”‚
â”‚  â””â”€ Test: no_chunking, recursive(512), preserve_tables(2048)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚  For each dataset:
           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  â”‚  FOR EACH CHUNKING STRATEGY:                 â”‚
           â”‚  â”‚                                              â”‚
           â”‚  â”‚  1. Load corpus + queries + qrels            â”‚
           â”‚  â”‚  2. Apply chunking method                    â”‚
           â”‚  â”‚  3. Encode chunks with embedder              â”‚
           â”‚  â”‚  4. Retrieve top-k chunks per query          â”‚
           â”‚  â”‚  5. Aggregate chunks â†’ documents             â”‚
           â”‚  â”‚  6. Compute NDCG@10                          â”‚
           â”‚  â”‚  7. Store results                            â”‚
           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“Š EVALUATION & COMPARISON                           â”‚
â”‚                                                                          â”‚
â”‚  For each dataset:                                                      â”‚
â”‚  â”œâ”€ Compare all tested strategies                                       â”‚
â”‚  â”œâ”€ Select best strategy (max NDCG@10)                                  â”‚
â”‚  â””â”€ Calculate improvement vs baseline                                   â”‚
â”‚                                                                          â”‚
â”‚  Across all datasets:                                                   â”‚
â”‚  â”œâ”€ Average NDCG@10 with optimal per-dataset config                     â”‚
â”‚  â”œâ”€ Compare with uniform chunking baseline                              â”‚
â”‚  â””â”€ Analyze method effectiveness                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸ’¾ OUTPUT GENERATION                                â”‚
â”‚                                                                          â”‚
â”‚  1. best_chunking_config_per_dataset.json                               â”‚
â”‚     {                                                                    â”‚
â”‚       "tatqa": {"method": "no_chunking", "ndcg_10": 0.4935},           â”‚
â”‚       "financebench": {"method": "recursive", "chunk_size": 512, ...}   â”‚
â”‚     }                                                                    â”‚
â”‚                                                                          â”‚
â”‚  2. dataset_chunking_method_mapping.json                                â”‚
â”‚     â†’ Simplified config for production                                  â”‚
â”‚                                                                          â”‚
â”‚  3. Chunked corpora: tatqa_corpus_chunked_optimal.jsonl, ...           â”‚
â”‚     â†’ Pre-chunked with optimal strategy                                 â”‚
â”‚                                                                          â”‚
â”‚  4. Visualizations:                                                     â”‚
â”‚     â”œâ”€ Performance by dataset (bar chart)                               â”‚
â”‚     â”œâ”€ Improvement vs baseline (bar chart)                              â”‚
â”‚     â”œâ”€ Dataset Ã— Method heatmap                                         â”‚
â”‚     â”œâ”€ Method performance comparison                                    â”‚
â”‚     â””â”€ Strategy distribution                                            â”‚
â”‚                                                                          â”‚
â”‚  5. Reports:                                                            â”‚
â”‚     â”œâ”€ dataset_specific_evaluation_report.txt                           â”‚
â”‚     â””â”€ dataset_specific_chunking_results.csv                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸš€ PRODUCTION DEPLOYMENT                              â”‚
â”‚                                                                          â”‚
â”‚  Step 1: Load chunking configuration                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ with open('dataset_chunking_method_mapping.json') as f:       â”‚     â”‚
â”‚  â”‚     config = json.load(f)                                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â”‚  Step 2: For each dataset, load pre-chunked corpus                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ corpus_file = f'{dataset}_corpus_chunked_optimal.jsonl'       â”‚     â”‚
â”‚  â”‚ chunked_corpus = load_jsonl(corpus_file)                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â”‚  Step 3: Retrieve with chunks + aggregate to documents                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ # Encode query + chunks                                       â”‚     â”‚
â”‚  â”‚ similarities = cosine_similarity(query_emb, chunk_embs)       â”‚     â”‚
â”‚  â”‚                                                               â”‚     â”‚
â”‚  â”‚ # Aggregate by original_id (MAX score)                        â”‚     â”‚
â”‚  â”‚ doc_scores[chunk['original_id']] = max(...)                   â”‚     â”‚
â”‚  â”‚                                                               â”‚     â”‚
â”‚  â”‚ # Return top-k documents                                      â”‚     â”‚
â”‚  â”‚ return sorted(doc_scores.items(), ...)[:k]                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â”‚  âœ… Expected Improvements:                                              â”‚
â”‚  â”œâ”€ FinanceBench: +114% (0.34 â†’ 0.74)                                  â”‚
â”‚  â”œâ”€ TATQA: +3.5% (0.48 â†’ 0.49)                                         â”‚
â”‚  â”œâ”€ MultiHeirTT: +100-170% (0.15 â†’ 0.30-0.40)                          â”‚
â”‚  â””â”€ Overall: +15-27% average NDCG@10                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Key Decision Points

### 1. Table Detection
```
Is document a table?
â”œâ”€ YES â†’ Use preserve_tables or no_chunking
â””â”€ NO â†’ Use recursive chunking
```

### 2. Chunk Size Selection
```
Document length?
â”œâ”€ <1000 chars â†’ no_chunking or 512 chunks
â”œâ”€ 1000-3000 chars â†’ 512-1024 chunks  
â””â”€ >3000 chars (tables) â†’ 2048-4096 chunks or no_chunking
```

### 3. Aggregation Strategy
```
Multiple chunks per document?
â”œâ”€ YES â†’ Aggregate using MAX score (best empirical results)
â””â”€ NO â†’ Use chunk score directly
```

## ğŸ“ˆ Performance Expectations

| Dataset | Current | Expected | Gain | Strategy |
|---------|---------|----------|------|----------|
| FinanceBench | 0.3439 | 0.7362 | +114% | recursive(512) âœ… PROVEN |
| TATQA | 0.4768 | 0.4935 | +3.5% | no_chunking âœ… PROVEN |
| MultiHeirTT | 0.1467 | 0.30-0.40 | +100-170% | preserve_tables(4096) ğŸ¯ TARGET |
| ConvFinQA | 0.4858 | 0.50-0.53 | +3-9% | preserve_tables(2048) |
| FinQA | 0.4570 | 0.47-0.50 | +3-9% | preserve_tables(2048) |
| FINDER | 0.3953 | 0.39-0.42 | 0-6% | no_chunking or recursive(512) |
| FinQABench | 0.8662 | 0.86-0.88 | 0-2% | no_chunking (already optimal) |
| **OVERALL** | **0.4949** | **0.57-0.63** | **+15-27%** | **Per-dataset config** |

---

**Note**: This is an ITERATIVE process. Run evaluation â†’ analyze results â†’ refine strategies â†’ re-run if needed.
