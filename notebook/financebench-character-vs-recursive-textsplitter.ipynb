{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "from sentence_transformers import CrossEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to Python path to find the financerag package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory (FinanceRAG root) to the path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "records = []\n",
    "\n",
    "file_path = os.path.join(data_dir, \"financebench_corpus.jsonl\", \"corpus.jsonl\")\n",
    "with open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data[\"source_file\"] = \"financebench_corpus.jsonl\"\n",
    "        records.append(data)\n",
    "\n",
    "# Convert all records to a pandas DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "4\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "# unique records\n",
    "print(len(df['_id'].unique()))\n",
    "\n",
    "# null records\n",
    "print(len(df.isna().sum()))\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates('_id',inplace=True)\n",
    "print(len(df['_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd2af2336</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>6) Africa, Middle East and South Asia (AMESA),...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd2acf5c0</td>\n",
       "      <td>BOEING_2022_10K</td>\n",
       "      <td>We derive a significant portion of our revenue...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd2ad12e4</td>\n",
       "      <td>COCACOLA_2022_10K</td>\n",
       "      <td>THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd2af3272</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>Note 3 Restructuring and Impairment Charges\\n2...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd2ade412</td>\n",
       "      <td>LOCKHEEDMARTIN_2020_10K</td>\n",
       "      <td>Table of Contents\\nLockheed Martin Corporation...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id                    title  \\\n",
       "0  dd2af2336         PEPSICO_2022_10K   \n",
       "1  dd2acf5c0          BOEING_2022_10K   \n",
       "2  dd2ad12e4        COCACOLA_2022_10K   \n",
       "3  dd2af3272         PEPSICO_2022_10K   \n",
       "4  dd2ade412  LOCKHEEDMARTIN_2020_10K   \n",
       "\n",
       "                                                text  \\\n",
       "0  6) Africa, Middle East and South Asia (AMESA),...   \n",
       "1  We derive a significant portion of our revenue...   \n",
       "2  THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...   \n",
       "3  Note 3 Restructuring and Impairment Charges\\n2...   \n",
       "4  Table of Contents\\nLockheed Martin Corporation...   \n",
       "\n",
       "                 source_file  \n",
       "0  financebench_corpus.jsonl  \n",
       "1  financebench_corpus.jsonl  \n",
       "2  financebench_corpus.jsonl  \n",
       "3  financebench_corpus.jsonl  \n",
       "4  financebench_corpus.jsonl  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Texts to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SIZES = [64, 128, 256, 368, 512]\n",
    "RECURSIVE_SIZES = CHAR_SIZES\n",
    "RECURSIVE_OVERLAP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowdict_iter(df: pd.DataFrame):\n",
    "    cols = list(df.columns)\n",
    "    for vals in df.itertuples(index=False, name=None):\n",
    "        yield dict(zip(cols, vals))\n",
    "\n",
    "def _coerce_text(x: Optional[str]) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "def _emit_rows(base_row: Dict, splitter_name: str, chunk_size: int, chunk_overlap: int, chunks: List[str]) -> List[Dict]:\n",
    "    return [\n",
    "        {\n",
    "            **base_row,\n",
    "            \"splitter\": splitter_name,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": ch,\n",
    "        }\n",
    "        for i, ch in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "def _chunk_all_rows_with_splitter(df: pd.DataFrame, splitter, splitter_name: str, size: int, overlap: int) -> List[Dict]:\n",
    "    rows: List[Dict] = []\n",
    "    for rd in rowdict_iter(df):\n",
    "        base = {\n",
    "            \"_id\": rd.get(\"_id\"),\n",
    "            \"title\": rd.get(\"title\"),\n",
    "            \"source_file\": rd.get(\"source_file\"),\n",
    "        }\n",
    "        text = _coerce_text(rd.get(\"text\", \"\"))\n",
    "        if not text:\n",
    "            continue\n",
    "        chunks = splitter.split_text(text)\n",
    "        rows.extend(_emit_rows(base, f\"{splitter_name}_{size}\", size, overlap, chunks))\n",
    "    return rows\n",
    "\n",
    "def make_all_chunks_with_docs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.index.name and df.index.name not in df.columns:\n",
    "        df = df.reset_index()\n",
    "\n",
    "    all_rows: List[Dict] = []\n",
    "    steps = [\"character\", \"recursive\"]\n",
    "\n",
    "    with tqdm(total=len(steps), desc=\"Chunking pipeline\", ncols=100) as pbar:\n",
    "        for size in CHAR_SIZES:\n",
    "            s = CharacterTextSplitter(chunk_size=size, chunk_overlap=0)\n",
    "            all_rows.extend(_chunk_all_rows_with_splitter(df, s, \"character\", size, 0))\n",
    "        pbar.update(1)\n",
    "\n",
    "        for size in RECURSIVE_SIZES:\n",
    "            s = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=RECURSIVE_OVERLAP)\n",
    "            all_rows.extend(_chunk_all_rows_with_splitter(df, s, \"recursive\", size, RECURSIVE_OVERLAP))\n",
    "        pbar.update(1)\n",
    "\n",
    "    chunks_df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    cols = [\"_id\", \"title\", \"source_file\", \"splitter\", \"chunk_size\", \"chunk_overlap\", \"chunk_index\", \"chunk_text\"]\n",
    "    chunks_df = chunks_df[[c for c in cols if c in chunks_df.columns] + [c for c in chunks_df.columns if c not in cols]]\n",
    "    return chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd2af2336</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>6) Africa, Middle East and South Asia (AMESA),...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd2acf5c0</td>\n",
       "      <td>BOEING_2022_10K</td>\n",
       "      <td>We derive a significant portion of our revenue...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd2ad12e4</td>\n",
       "      <td>COCACOLA_2022_10K</td>\n",
       "      <td>THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd2af3272</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>Note 3 Restructuring and Impairment Charges\\n2...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd2ade412</td>\n",
       "      <td>LOCKHEEDMARTIN_2020_10K</td>\n",
       "      <td>Table of Contents\\nLockheed Martin Corporation...</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id                    title  \\\n",
       "0  dd2af2336         PEPSICO_2022_10K   \n",
       "1  dd2acf5c0          BOEING_2022_10K   \n",
       "2  dd2ad12e4        COCACOLA_2022_10K   \n",
       "3  dd2af3272         PEPSICO_2022_10K   \n",
       "4  dd2ade412  LOCKHEEDMARTIN_2020_10K   \n",
       "\n",
       "                                                text  \\\n",
       "0  6) Africa, Middle East and South Asia (AMESA),...   \n",
       "1  We derive a significant portion of our revenue...   \n",
       "2  THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...   \n",
       "3  Note 3 Restructuring and Impairment Charges\\n2...   \n",
       "4  Table of Contents\\nLockheed Martin Corporation...   \n",
       "\n",
       "                 source_file  \n",
       "0  financebench_corpus.jsonl  \n",
       "1  financebench_corpus.jsonl  \n",
       "2  financebench_corpus.jsonl  \n",
       "3  financebench_corpus.jsonl  \n",
       "4  financebench_corpus.jsonl  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.94it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks_df = make_all_chunks_with_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitter\n",
       "character_512     284\n",
       "character_368     310\n",
       "character_256     346\n",
       "character_128     440\n",
       "character_64      572\n",
       "recursive_512     628\n",
       "recursive_368     846\n",
       "recursive_256    1246\n",
       "recursive_128    2671\n",
       "recursive_64     5766\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df.groupby(\"splitter\").size().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>source_file</th>\n",
       "      <th>splitter</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd2af2336</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "      <td>character_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6) Africa, Middle East and South Asia (AMESA),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd2acf5c0</td>\n",
       "      <td>BOEING_2022_10K</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "      <td>character_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We derive a significant portion of our revenue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd2ad12e4</td>\n",
       "      <td>COCACOLA_2022_10K</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "      <td>character_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd2af3272</td>\n",
       "      <td>PEPSICO_2022_10K</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "      <td>character_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Note 3 Restructuring and Impairment Charges\\n2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd2ade412</td>\n",
       "      <td>LOCKHEEDMARTIN_2020_10K</td>\n",
       "      <td>financebench_corpus.jsonl</td>\n",
       "      <td>character_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Table of Contents\\nLockheed Martin Corporation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id                    title                source_file  \\\n",
       "0  dd2af2336         PEPSICO_2022_10K  financebench_corpus.jsonl   \n",
       "1  dd2acf5c0          BOEING_2022_10K  financebench_corpus.jsonl   \n",
       "2  dd2ad12e4        COCACOLA_2022_10K  financebench_corpus.jsonl   \n",
       "3  dd2af3272         PEPSICO_2022_10K  financebench_corpus.jsonl   \n",
       "4  dd2ade412  LOCKHEEDMARTIN_2020_10K  financebench_corpus.jsonl   \n",
       "\n",
       "       splitter  chunk_size  chunk_overlap  chunk_index  \\\n",
       "0  character_64          64              0            0   \n",
       "1  character_64          64              0            0   \n",
       "2  character_64          64              0            0   \n",
       "3  character_64          64              0            0   \n",
       "4  character_64          64              0            0   \n",
       "\n",
       "                                          chunk_text  \n",
       "0  6) Africa, Middle East and South Asia (AMESA),...  \n",
       "1  We derive a significant portion of our revenue...  \n",
       "2  THE COCA-COLA COMPANY AND SUBSIDIARIES\\nCONSOL...  \n",
       "3  Note 3 Restructuring and Impairment Charges\\n2...  \n",
       "4  Table of Contents\\nLockheed Martin Corporation...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_DIR = \"./vectordbs\"\n",
    "\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"batch_size\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_for_splitter(\n",
    "    chunks_df: pd.DataFrame,\n",
    "    splitter_name: str,\n",
    "    parent_dir: str = PARENT_DIR,\n",
    ") -> Chroma:\n",
    "    sub = chunks_df[chunks_df[\"splitter\"] == splitter_name].copy()\n",
    "    sub = sub[sub[\"chunk_text\"].notna() & (sub[\"chunk_text\"].str.len() > 0)]\n",
    "    texts = sub[\"chunk_text\"].astype(str).tolist()\n",
    "    metadatas: List[Dict] = sub[[\"_id\", \"source_file\", \"splitter\", \"chunk_index\"]].to_dict(\"records\")\n",
    "\n",
    "    persist_dir = os.path.join(parent_dir, f\"chroma_{splitter_name}\")\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    db = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=f\"col_{splitter_name}\"\n",
    "    )\n",
    "    db.persist()\n",
    "    return db\n",
    "\n",
    "def load_chroma(splitter_name: str, parent_dir: str = PARENT_DIR) -> Chroma:\n",
    "    persist_dir = os.path.join(parent_dir, f\"chroma_{splitter_name}\")\n",
    "    return Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=f\"col_{splitter_name}\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "splitters_to_build = sorted(chunks_df[\"splitter\"].unique().tolist())\n",
    "\n",
    "built = {}\n",
    "for sp in splitters_to_build:\n",
    "    print(f\"ðŸ”§ Building Chroma for splitter: {sp}\")\n",
    "    built[sp] = build_chroma_for_splitter(chunks_df, sp, parent_dir=PARENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = \"How does Boeing's effective tax rate in FY2022 compare to FY2021?\"\n",
    "splitters = [\"character_512\", \"recursive_512\"]\n",
    "\n",
    "def search_chroma(splitter):\n",
    "    db = Chroma(\n",
    "        persist_directory=f\"{PARENT_DIR}/chroma_{splitter}\",\n",
    "        collection_name=f\"col_{splitter}\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    return retriever.get_relevant_documents(search_text)\n",
    "\n",
    "for sp in splitters:\n",
    "    print(f\"\\n=== {sp} ===\")\n",
    "    for i, d in enumerate(search_chroma(sp), 1):\n",
    "        meta = d.metadata or {}\n",
    "        print(f\"[{i}] {meta.get('_id')} | {meta.get('source_file')} | {meta.get('chunk_index')}\")\n",
    "        print(d.page_content[:250].replace(\"\\n\", \" \") + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Top 5 results with reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "def rerank_top_k(query, docs, top_n=5):\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    out = []\n",
    "    for d, s in ranked:\n",
    "        meta = d.metadata or {}\n",
    "        out.append({\n",
    "            \"score\": float(s),\n",
    "            \"_id\": meta.get(\"_id\"),\n",
    "            \"source_file\": meta.get(\"source_file\"),\n",
    "            \"chunk_index\": meta.get(\"chunk_index\"),\n",
    "            \"text\": d.page_content\n",
    "        })\n",
    "    return out\n",
    "\n",
    "for sp in splitters: \n",
    "    print(f\"\\n===== {sp} | Reranked Top 5 =====\")\n",
    "    retrieved = search_chroma(sp)\n",
    "    top5 = rerank_top_k(search_text, retrieved, top_n=5)\n",
    "    for i, r in enumerate(top5, 1):\n",
    "        print(f\"[{i}] score={r['score']:.3f}  id={r['_id']}  chunk={r['chunk_index']}  file={r['source_file']}\")\n",
    "        print(r[\"text\"][:300].replace(\"\\n\", \" \") + (\"...\" if len(r[\"text\"]) > 300 else \"\"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalaution vs Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "records = []\n",
    "\n",
    "file_path = os.path.join(data_dir, \"financebench_queries.jsonl.gz\")\n",
    "with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        records.append(data)\n",
    "\n",
    "# Convert all records to a pandas DataFrame\n",
    "df_test = pd.DataFrame(records)\n",
    "df_test = df_test[['_id','text']]\n",
    "df_test.columns = ['query_id','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('FinanceBench_qrels.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_eval['query_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.merge(df_test,on='query_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTERS = splitters_to_build\n",
    "TOP_K_RETRIEVE = 10\n",
    "TOP_K_RERANK = 5\n",
    "\n",
    "def load_chroma(splitter):\n",
    "    return Chroma(\n",
    "        persist_directory=f\"{PARENT_DIR}/chroma_{splitter}\",\n",
    "        collection_name=f\"col_{splitter}\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "def retrieve_docs(db, query_text, k=TOP_K_RETRIEVE):\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever.get_relevant_documents(query_text)\n",
    "\n",
    "def rerank_docs(query_text, docs, top_n=TOP_K_RERANK):\n",
    "    pairs = [(query_text, d.page_content) for d in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_n]\n",
    "\n",
    "results = []\n",
    "\n",
    "for splitter in SPLITTERS:\n",
    "    print(f\"\\nðŸ”Ž Evaluating splitter: {splitter}\")\n",
    "    db = load_chroma(splitter)\n",
    "    labels = []\n",
    "\n",
    "    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval)):\n",
    "        corpus_id = row[\"corpus_id\"]\n",
    "        query_text = row[\"text\"]\n",
    "\n",
    "        retrieved_docs = retrieve_docs(db, query_text, k=TOP_K_RETRIEVE)\n",
    "\n",
    "        reranked = rerank_docs(query_text, retrieved_docs, top_n=TOP_K_RERANK)\n",
    "\n",
    "        top_ids = [d.metadata.get(\"_id\") for d, _ in reranked if d.metadata and \"_id\" in d.metadata]\n",
    "\n",
    "        label = 1 if corpus_id in top_ids else 0\n",
    "        labels.append(label)\n",
    "\n",
    "    df_eval[f\"label_{splitter}_rerank\"] = labels\n",
    "    results.append((splitter, sum(labels), len(labels), sum(labels) / len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(results, columns=[\"splitter\", \"correct\", \"total\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df = chunks_df.groupby(\"splitter\").size().sort_values(ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df.columns = ['splitter','chunkSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = chunks_df.merge(df_summary[['splitter','accuracy']],on='splitter',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['accuracy'] = round(df_summary['accuracy'] * 100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['splitter_type'] = df_summary['splitter'].apply(lambda x: 'character' if 'character' in x else 'recursive')\n",
    "df_summary['char_size'] = df_summary['splitter'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "sns.relplot(data=df_summary, x='chunkSize', y='accuracy', hue='char_size',\n",
    "            col='splitter_type', kind='scatter', palette='viridis', height=4, aspect=1.2)\n",
    "\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Splitters\n",
    "\n",
    "Accuracy remains stable between 72â€“76% across all chunk sizes (64â€“512).\n",
    "\n",
    "Shows minimal sensitivity to chunk size â€” even smaller chunks retain strong performance.\n",
    "\n",
    "Indicates consistent context preservation and balanced granularity.\n",
    "\n",
    "Ideal for CPU-based or small-scale RAG systems where simplicity and stability are preferred.\n",
    "\n",
    "#### Recursive Splitters\n",
    "\n",
    "Accuracy drops sharply for smaller chunk sizes (64 â†’ ~32%).\n",
    "\n",
    "Performs best at larger chunk sizes (368â€“512) with accuracy near 76%.\n",
    "\n",
    "Shows high sensitivity to chunk size â€” small chunks cause excessive fragmentation.\n",
    "\n",
    "Likely struggles with structured content (e.g., tables, lists) when chunks are too fine-grained.\n",
    "\n",
    "#### Overall Insights\n",
    "\n",
    "CharacterTextSplitter is more robust and reliable across configurations.\n",
    "\n",
    "RecursiveCharacterTextSplitter can perform well but requires careful tuning of chunk size.\n",
    "\n",
    "For general RAG use cases, character splitters provide better trade-off between accuracy, speed, and stability."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9767247,
     "sourceId": 85594,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "financerag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
